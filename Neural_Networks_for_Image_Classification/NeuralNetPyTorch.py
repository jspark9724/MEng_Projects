# -*- coding: utf-8 -*-
"""NNPyTorch_Jun.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/125RIOepkQFV9yi-MMWEg3Bx4fBLLSkKq
"""

# Objectives:
# This assignment investigates the classification performance of neural networks.
# The task involves implementing, training, and evaluating neural network models
# using PyTorch on the notMNIST dataset. 

# Notes:
# - Use the notMNIST.npz dataset and PyTorch documentation for utility functions.
# - Train your neural network using a GPU for faster training.



import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

import matplotlib.pyplot as plt
import numpy as np

# Function for loading notMNIST Dataset
def loadData(datafile = "notMNIST.npz"):
    with np.load(datafile) as data:
        Data, Target = data["images"].astype(np.float32), data["labels"]
        np.random.seed(521)
        randIndx = np.arange(len(Data))
        np.random.shuffle(randIndx)
        Data = Data[randIndx] / 255.0
        Target = Target[randIndx]
        trainData, trainTarget = Data[:10000], Target[:10000]
        validData, validTarget = Data[10000:16000], Target[10000:16000]
        testData, testTarget = Data[16000:], Target[16000:]
    return trainData, validData, testData, trainTarget, validTarget, testTarget

# Custom Dataset class. 
class notMNIST(Dataset):
    def __init__(self, annotations, images, transform=None, target_transform=None):
        self.img_labels = annotations
        self.imgs = images
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        image = self.imgs[idx]
        label = self.img_labels[idx]
        if self.transform:
            image = self.transform(image)
        if self.target_transform:
            label = self.target_transform(label)
        return image, label

# BATCH SIZE = 32
# F is an abbreviated import name of torch.nn.functional.
# nn is an abbreviated import name of torch.nn


# Part 1: Fully Connected Neural Networks
# - Implement a Fully Connected Neural Network using PyTorch.
# - The neural network architecture includes multiple fully connected layers
#   with ReLU activations and a dropout layer.

#Define CNN
class CNN(nn.Module):
    def __init__(self, drop_out_p=0.0):
        super(CNN, self).__init__()
        #TODO
        #DEFINE YOUR LAYERS HERE

        # Convolutional Layer 1
        self.con1 = nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 4)
        # Batch Norm Layer 1
        self.batch1 = nn.BatchNorm2d(num_features = 32)
        # Max Pooling Operation wiht kernal size 2x2
        self.maxpool = nn.MaxPool2d(2, 2)
        # Convolutional Layer 2
        self.con2 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 4)
        # Batch Norm Layer 2
        self.batch2 = nn.BatchNorm2d(num_features = 64)
        # Dropout Layer 
        self.dropout = nn.Dropout(p=drop_out_p)
        # Fully Connected Layer 1
        self.fc1 = torch.nn.Linear(1024, 784)
        # Fully Connected Layer 2
        self.fc2 = nn.Linear(784, 10)

    def forward(self, x):
        #TODO
        #DEFINE YOUR FORWARD FUNCTION HERE
        
        # Apply ReLU Activation Function to Convolutional Layer 1, and then apply batch norm layer 1.
        x = self.batch1(F.relu(self.con1(x)))
        # Apply Max Pooling operation with kernal size of 2x2
        x = self.maxpool(x)
        # Apply ReLU Activation Function to Convolutional Layer 2, and then apply batch norm layer 2.
        x = self.batch2(F.relu(self.con2(x)))
        # Apply Max Pooling operation with kernal size of 2x2
        x = self.maxpool(x)
        # Flatten Operation to transform the previous hidden layer to batch of 1D arrays
        x = x.flatten(start_dim = 1)  # The size of matrix is now 32 x 1024
        # Add a Dropout Layer
        x = self.dropout(x)
        # Apply Fully connected layer 1
        x = self.fc1(x)
        # Apply ReLU Activation Function
        x = F.relu(x)  # The size is now 32 x 784
        # Apply a fully connected layer without SoftMax activation
        # logits => 32x10
        out = self.fc2(x)  # The size is now 32 x 10

        return out


# Part 2: Convolutional Neural Networks
# - Implement a Convolutional Neural Network using PyTorch.
# - The neural network architecture includes convolutional layers, ReLU activations,
#   batch normalization, and max pooling operations.

#Define FNN
class FNN(nn.Module):
    def __init__(self, drop_out_p=0.0):
        super(FNN, self).__init__()
        # DEFINE YOUR LAYERS HERE
        # the sizes of hidden layers all equal to 10
        self.fc1 = torch.nn.Linear(28*28,10)
        self.fc2 = torch.nn.Linear(10,10)
        # dropout layer
        self.dropout = nn.Dropout(p=drop_out_p)

    def forward(self, x):
        # DEFINE YOUR FORWARD FUNCTION HERE
        # input layer
        x = torch.flatten(x,start_dim=1) #[BATCH_SIZE,28*28=784]
        # hidden layer
        x = self.fc1(x) #[batch_size,10]
        # activiation function of the hidden layer, relu
        x = F.relu(x)  #[batch_size,10]
        # hidden layer
        x = self.fc2(x) #[batch_size,10]
        # activiation function of the hidden layer, relu
        x = F.relu(x)  #[batch_size,10]
        # dropout layer
        x = self.dropout(x)
        # hidden layer
        x = self.fc2(x) #[batch_size,10]
        # activiation function of the hidden layer, softmax
        out = F.softmax(x,dim=-1)  #[batch_size,10]

        return out

# Commented out IPython magic to ensure Python compatibility.
# Compute accuracy
def get_accuracy(model, dataloader):
    
    model.eval()
    device = next(model.parameters()).device
    accuracy = 0.0
    total = 0.0
    num_corrects  = 0
    
    with torch.no_grad():
        for data in dataloader:
            images, labels = data
            images = images.to(device)
            labels = labels.to(device) 
            # TODO
            # Return the accuracy
            #output of using model
            scores = model(images)
            #using binary classification to predict
            predicts = (scores >= 0.5)*1
            #add the correct predictions
            num_corrects += torch.sum(predicts.argmax(1) == labels)
            #calculate the accuracy
            accuracy =  num_corrects / len(dataloader.dataset) 



    return accuracy

# Commented out IPython magic to ensure Python compatibility.
def train(model, device, learning_rate, weight_decay, train_loader, val_loader, test_loader, num_epochs=50, verbose=False):
  #TODO
  # Define your cross entropy loss function here 
  # Use cross entropy loss
  criterion = nn.CrossEntropyLoss() 
  
  #TODO
  # Define your optimizer here
  # Use AdamW optimizer, set the weights, learning rate and weight decay argument.
  optimizer = torch.optim.Adam(model.parameters(), lr= learning_rate, weight_decay = weight_decay)

  acc_hist = {'train':[], 'val':[], 'test': []}

  for epoch in range(num_epochs):
    model = model.train()
    ## training step
    for i, (images, labels) in enumerate(train_loader):
      images = images.to(device)
      labels = labels.to(device)

        # TODO
        # Follow the step in the tutorial
        ## forward + backprop + loss
      #output of using model
      output = model(images)
      #loss using crossEntropy
      loss = criterion(output, labels) 
      #backwardprogation
      loss.backward()
      optimizer.step()
      #set the weightâ€™s gradients to zero
      optimizer.zero_grad()
        

        ## update model params
        

    model.eval()
    acc_hist['train'].append(get_accuracy(model, train_loader))
    acc_hist['val'].append(get_accuracy(model, val_loader))
    acc_hist['test'].append(get_accuracy(model, test_loader))
    
    if verbose:
      print('Epoch: %d | Train Accuracy: %.2f | Validation Accuracy: %.2f | Test Accuracy: %.2f' \
          %(epoch, acc_hist['train'][-1], acc_hist['val'][-1], acc_hist['test'][-1]))

  return model, acc_hist

# Part 3: Model Training and Experiments
# - Implement functions for model training, evaluation, and experiments.
# - Use cross-entropy loss, Adam optimizer, and L2 regularization during training.
# - Perform experiments to compare different architectures, dropout rates, and
#   weight decay effects on the CNN model's performance.

def experiment(model_type='CNN', learning_rate=0.0001, dropout_rate=0.5, weight_decay=0.01, num_epochs=50, verbose=False):
  # Use GPU if it is available.
  device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

  # Inpute Batch size:
  BATCH_SIZE = 32

  # Convert images to tensor
  transform = transforms.Compose(
      [transforms.ToTensor()])

  # Get train, validation and test data loader.
  trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()

  train_data = notMNIST(trainTarget, trainData, transform=transform)
  val_data = notMNIST(validTarget, validData, transform=transform)
  test_data = notMNIST(testTarget, testData, transform=transform)


  train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)
  val_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)
  test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)

  # Specify which model to use
  if model_type == 'CNN':
    model = CNN(dropout_rate)
  elif model_type == 'FNN':
    model = FNN(dropout_rate)

  
  # Loading model into device
  model = model.to(device)
  criterion = nn.CrossEntropyLoss()
  model, acc_hist = train(model, device, learning_rate, weight_decay, train_loader, val_loader, test_loader, num_epochs=num_epochs, verbose=verbose)
  
  # Release the model from the GPU (else the memory wont hold up)
  model.cpu()

  return model, acc_hist

def compare_arch(model_type='CNN'):
  #train CNN
  if model_type == 'CNN':
    model, A = experiment(model_type='CNN', learning_rate=0.0001, dropout_rate=0.0, weight_decay=0.0, num_epochs=50, verbose=False)
  #train FNN
  elif model_type == 'FNN':
    model, A = experiment(model_type='FNN', learning_rate=0.0001, dropout_rate=0.0, weight_decay=0.0, num_epochs=50, verbose=False)
  #train accuracy and test accuracy
  train_list = []
  for i in range (len(A['train'])):
    #change to cpu
    train_list.append(A['train'][i].cpu())
  test_list = []
  for j in range (len(A['test'])):
    test_list.append(A['test'][j].cpu())

  C = np.array(train_list)
  D = np.array(test_list)
  #plot train and test accuracy for CNN and FNN
  figure = plt.plot(C, label='train accuracy')
  figure = plt.plot(D, label='test accuracy')
  plt.legend()
  if model_type == 'CNN':
    plt.title('CNN')
  elif model_type == 'FNN':
    plt.title('FNN')
  return figure

CNN_plot = compare_arch(model_type='CNN')

FNN_plot = compare_arch(model_type='FNN')

def compare_dropout(dropoutrate):
  #train CNN model with different dropout rate
  model, B = experiment(model_type='CNN', learning_rate=0.0001, dropout_rate=dropoutrate, weight_decay=0.0, num_epochs=50, verbose=False)
  #train accuracy and test accuracy
  train_list = []
  for i in range (len(B['train'])):
    train_list.append(B['train'][i].cpu())
  test_list = []
  for j in range (len(B['test'])):
    test_list.append(B['test'][j].cpu())

  C = np.array(train_list)
  D = np.array(test_list)
  #plot the train and test accuracy
  figure = plt.plot(C, label='train accuracy')
  figure = plt.plot(D, label='test accuracy')
  plt.legend()
  plt.title('CNN')
  return figure

dropout_rate_1 = compare_dropout(0.5)

dropout_rate_2 = compare_dropout(0.8)

dropout_rate_3 = compare_dropout(0.95)

def compare_l2(weightdecay):
  #train CNN with different weight decay
  model, Z = experiment(model_type='CNN', learning_rate=0.0001, dropout_rate=0.0, weight_decay=weightdecay, num_epochs=50, verbose=False)
  #test accuracy and train accuracy
  train_list = []
  for i in range (len(Z['train'])):
    train_list.append(Z['train'][i].cpu())
  test_list = []
  for j in range (len(Z['test'])):
    test_list.append(Z['test'][j].cpu())

  C = np.array(train_list)
  D = np.array(test_list)
  #plot train and test accuracy
  figure = plt.plot(C, label='train accuracy')
  figure = plt.plot(D, label='test accuracy')
  plt.legend()
  plt.title('CNN')
  return figure

l2_1 = compare_l2(0.1)

l2_2 = compare_l2(1.0)

l2_3 = compare_l2(10.0)